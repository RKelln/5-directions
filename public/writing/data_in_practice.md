Welcome to
# Data in Practice

Notes: 
Welcome to Data in Practice. In previous tutorials we've covered the basic foundations and history of mathematics and computer science, and investigated neural nets and their history. We considered what it means to make datasets ethically, in a 'Good Way'. 

In this tutorial we'll add more practical details about finding, making and curating datasets in a Good Way.


# Data in Practice

* Selection
* Preprocessing
* Dataset discovery
* Dataset types:
  * Times series
  * Image and video
  * Text
  * Audio
* Data pitfalls
* Making things ethically
  * FAIR, CARE & Local Contexts
  * Maintenance

[Companion Data in Practice details document](./companion.html)

Notes:
We'll start off by talking about how to select and augment a dataset and the work that goes into preparing the dataset before it can be safely used as training data. Then we'll talk about tools for finding existing datasets.

We'll look at the different types of datasets and discuss bias and fair distribution in more detail, as well as tools to explore and handle those issues.

Next I'll revisit how to make datasets ethically with a focus on care work and maintenance of the dataset.

Our goal is to get you started on finding or recording your own data, then cleaning and curating it so that it can be used in an ML project done in collaboration with ML engineers. We'll learn how to do this in a Good Way, and share it ethically with others.

This tutorial has a companion document that includes extra details that are easier to read and reference. I'll introduce the key concepts here, so you have a better idea of what is available in the document. I encourage you to use that document as a reference and jumping off point for further learning.

---

# Selection

* How much data?
* Synthetic augmentation

Notes:
How much data do you need? There is no hard guide to this, but typically there is a sweet spot where performance gains dramatically but levels off after that. That point in real world datasets can be difficult to determine, but people have suggested the "rule of 10": that you want roughly 10 times as many examples as there are parameters (i.e. neurons) in your model. (Most modern ML frameworks will be able to give you a parameter count of your model.) However, text, image, and video data which is very common in generative models usually requires more than this because of the noise in the data.

For classification, you want enough data per "class" and roughly balanced data per classes, with recommendations for image classification at around 1000 images per class.

If you don't have enough data there some options, we'll get into specifics for the different types of datasets but the basic options are:



* Augmentation
* Transfer learning
* Synthetic data

Augmentation means applying transformations to your data, usually dynamically during training. For images this almost always includes horizontal flips, as most images are equivalent when mirrored. 


Transfer learning involves finding and using an existing fully trained network then using that to help train a new one. This can be done through retraining the model or using it as a teacher for your new student model. In the first case, sometimes the pretrained model has some layers removed, generally the highest level ones. Model layers start from generic to task/data specific in the final layer. For example, if you are creating an image model you could take an existing image classification model trained on millions of images and replace the last layer (which is doing the classification) with new layers. You could remove a few more layers if you just wanted to keep the lower-level feature detection.


Another approach is synthetic data, in which at least some portion of the data is generated by machine. This is more common in basic ML research, rather than in generative models. Synthetic data is growing in popularity in commercial settings, since it allows a business to de-bias and increase privacy of their user data. Synthetic data for 3D datasets is also growing in popularity as labelling 3D data is difficult in real world settings. 


Synthetic data may be an interesting approach in generative art making as it allows for interesting combinations of data. For example, in one of my pieces I used a style-transfer technique with multiple different styles applied to still images extracted from video and then used those stylized images to train a generative model that could produce animations that moved smoothly from one style to another.


Artists like Hans Brouwer use similar techniques to solve the problem of building an interesting dataset starting from just a few images. In practice this might be a few cycles of training and then creating a first draft of output, applying some transformation to that output, and then feeding those transformed images back through the model again. He repeats this loop a number of times, increasing the number of good training images each time.

### Credits
* https://machinelearningmastery.com/transfer-learning-for-deep-learning/
* https://wavefunk.xyz/blog/lakspe 
* https://wavefunk.xyz/blog/rhodops

---

# Preprocessing

* Cleaning
* Balancing
* Feature scaling
* Splitting
  
Notes:
Raw data is a mess and an old adage in computer science is "garbage in, garbage out". So carefully cleaning and curating your dataset is critical for the training process. 

Some basic tasks include cleaning the data, balancing the data between classes (for example, you wouldn't want 100 times as many dogs as cats), checking the range of feature values and ensuring all the data has roughly similar ranges or sizes.

Then you may want to split your data into training, validation and test sets. By splitting your data and holding some it back from training you are better able to test when your model is generalizing well to data other than what it was trained on.


## Cleaning 

* Missing values
* Duplicate values
* Bad feature values
* Wrong type of value
* Bad labels
* Consistent timestamps/dates

Data, particularly anything entered by humans, is notoriously riddled with errors. Data entries with multiple values may have missing values or regularly timed entries may be missing data points when the system went down, etc. Entries may be duplicated or transposed.

The values may have misplaced decimals or an extra zero or other number added. Some entries might have text where only numbers should be.

With labelled data, such as labelled images, it is very common to have incorrectly labelled data. If you use crowd-working online services for labels then you need to have a variety of data integrity checks. For example, have multiple answers from different workers to compare for agreement.

Another common error is inconsistent timestamps, such as year-month-day vs month-day-year. Also be on the lookout for timezone problems, times and dates may be set to a particular timezone (where the data was recorded) or may default to UTC (with zero timezone offset). If the time ends in a Z that generally indicates UTC time.

### Credits
* https://developers.google.com/machine-learning/data-prep/construct/collect/data-size-quality


## Balancing

Some datasets have vastly different proportions of the different classes of data they contain. For example, an animal classifier using images from the internet might have millions of photos of dogs and cats, but only a few of platypuses. 

If after training on the imbalanced dataset your results on the minority classes (i.e. the platypuses) are bad then you can balance them using downsampling and upweighting.

Downsampling means training on fewer of the majority cases. You can randomly sample from the majority to use a less imbalanced ratio, perhaps down to 10 to 1. Then you can upweight these downsampled majority examples - you can increase the importance of those examples in the calculation of the loss. Thus you end up with fewer of the majority class examples, so the model sees the minority classes more, but to compensate the majority examples change the weights more.

### Credits
* https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data


## Feature scaling

For time-series and other numerical data you may need to ensure that the data is roughly similar to get good results. Some machine learning algorithms are sensitive to scale and differences in scale between different inputs. Feature scaling transforms the data so that it works well with the algorithm you are using. Note that sometimes the differences in scale between variables captures important information, so scaling makes things worse.

Feature scaling mainly consists of two practices: normalization and standardization.

Normalization, also known a min-max scaling, is used when your data has few outliers and is approximately uniformly distributed (i.e. it has values in all parts of the range). Normalization shifts and rescales the values, so they range from 0 to 1 or -1 to 1. Most neural network approaches work best with normalized values.

Standardization centers values around the mean with a unit standard deviation. In less mathematical language, it shifts the data into a bell curve with the peak centered on zero. Outliers in the data will remain, so usually this is used for data that already follows a Gaussian or normal distribution.

There aren't hard rules about when to use or not use types of feature scaling, but you can try raw, normalized, and standardized data to see which gives the best results.

### Credits
* https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/
* https://developers.google.com/machine-learning/data-prep/transform/normalization
* https://en.wikipedia.org/wiki/Normal_distribution


<!-- .slide: data-audio-src="../audio/data/TODO.ogg" -->
## Splitting

* Training
* Validation
* Test

To examine how effective the network is at generalization you can partition or split the data you collect into three groups: training, validation and test. 

Training data is used in the training process and is seen many times by the network. This is the data used to generate errors and learn from. 

Validation data is used during training but only to check on how well the model handles data it hasn't seen before, the weights of the network are never updated in response to the validation data. Instead, this check is mainly used to know when to stop training - when the performance on the training data is good **and** the performance on the validation data is good. It is entirely possible for the performance on these to be very different.

Test data is used after the model is finished training to give it another set of data it has never been trained on and never been checked whether it generalizes well to. This is to get a rough idea of performance of the model "in the wild" on data that you didn't collect and this is the data used when researchers report their findings in papers. 

When models are used for art, splitting the data may be less important, as often the output will be visually inspected by the artist since the desired results are subjective. In addition, not using all your data for training by holding out data for validation and testing can make already small datasets even smaller. For art related projects you may only want to split 10% or so of the data to use for validation. Even then you may only need to do this if the model is receiving new data after the model has been trained. So, for example, generative models that are used to produced images that are used in the art do not generally need validation data, but something that generates images in real time from sensors during an installation may benefit from validation data.

---

# Dataset discovery

* [UCI](https://archive.ics.uci.edu)
* [Kaggle]https://kaggle.com/datasets
* [Google dataset search](https://datasetsearch.research.google.com)
* [OpenML](https://www.openml.org/search?type=data)
* [DataHub](https://datahub.io/search)
* [AWS open data registry](https://registry.opendata.aws/)
  
* [Library of Missing Datasets](https://github.com/MimiOnuoha/missin g-datasets)

These are good places to do general searches for datasets. A vast majority of the datasets you can find in these aggregators are for government, business or research purposes, so aren't directly applicable for art making, but certainly offer opportunities for associative and critical exploration.

Existing datasets will still often need processing, although it is common to find a raw dataset version and cleaned version.

I've also included Mimi Onuoha's project, _Library of Missing Datasets_ to give a sense of what data isn't being recorded and why that data is missing.

### Credits
* https://www.v7labs.com/blog/best-free-datasets-for-machine-learning
* https://pub.towardsai.net/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f
* https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research

---

# Time series data

Time series data records values (typically from some sort of sensor or digital process). This includes temperature and weather data, economic data, population data, and physiological data.

Generally if you're using time series data for an art project you are:


1. Using real-time sensor data
   * The main issue is handling sensor failure and other outliers in real time
2. Replaying recorded sensor data
   * The benefit here is that you can clean the data, otherwise similar to above
3. Predicting future data
   * Why is the prediction necessary vs recorded output ?

Notes:
Using real-time sensor data, where the main issue is handling sensor failure and other outliers in real-time. If instead you are replaying a recording of sensor data then you can clean the data. Otherwise you may be predicting or generating future data, but I'd question whether that prediction is necessary for the piece or if you could just use recorded output. In general avoiding real-time data and predictions will make your life easier. 

If you want to work with time series data you will likely need to work with a data scientist or learn some math, statistics and programming (which I encourage regardless of other collaborators). It may be less daunting than you think, as projects like [Auto_TS](https://github.com/AutoViML/Auto_TS) claim to allow for building models in "one line of code".


## Cleaning

Time series data can often have null or missing values. Depending on how many there are you can delete them or try to fill them in with averages or other approximations from surrounding data.

You will want to check the boundaries of the data - the highest and lowest values. Using this process you can also look for outliers. You'll need to investigate values that seem far too high or low compared to the rest of the data, this could be from sensor calibration errors or outright failures or just noise in the signal.


* Mean-reverting or explosive behaviour?
* Time trend?
* Seasonality or cycles?
* Structural breaks?

The first step of working with time series data is generally to visualize it in a graph plot. Depending on how much data there is this can be a useful way to check aspects of the data, along with statistical analysis.

Mean-reverting data returns to a value that is stable over time, and it is important to know if this is at 0 or some other value. This data is called stationary.

Conversely, the data may have a trend or non-stable shape. Data can have cycles or seasonality. You may want to de-trend the data (sometimes you can find seasonally adjusted versions) or use a model that handles seasonality.

The worst case scenario for time series data is probably structural breaks. These are large sudden shifts in the data. You'll likely need to have special models or otherwise transform the data so that it is comparable.

### Credits
* https://www.aptech.com/blog/introduction-to-the-fundamentals-of-time-series-data-and-analysis/


### OpenRefine

https://openrefine.org/

There are a few tools to help clean data, one of open source tools is [OpenRefine](https://openrefine.org/). It works with a variety of data formats and runs locally on your machine, but you use your web browser to interact with it. 

Particularly for data with human-entered text values, OpenRefine helps spot and fix inconsistent entries and mistakes. There are [additional extensions](https://github.com/FAIRDataTeam/OpenRefine-metadata-extension/) for OpenRefine that help integrate FAIR data principles.

When starting your own datasets it is important to collect metadata. You can find reference and example metadata for any type of data at [schema.org](https://schema.org/docs/schemas.html).

### Credits
* https://openrefine.org/
* https://docs.openrefine.org/
* https://github.com/FAIRDataTeam/OpenRefine-metadata-extension/
* https://schema.org/docs/schemas.html
* https://commons.wikimedia.org/wiki/File:OpenRefine_logo_color.png


## Existing datasets

* [Time Series Data Library (TSDL)](https://pkg.yangzhuoranyang.com/tsdl/)
* [GapMinder](https://www.gapminder.org/data/)
* [Our World in Data - Biodiversity](https://ourworldindata.org/biodiversity)
* [HYDE (History database of the Global Environment)](https://www.pbl.nl/en/image/links/hyde)
* [Pangeo Datastore](https://catalog.pangeo.io/)

Notes:
Time series data can be easily found in the general dataset searches, but there a few worth mentioning anyways, particularly if you have an environmental focus:
* TSDL was created by Rob Hyndman, Professor of Statistics at Monash University, and contains 650 datasets of a wide variety of types.
* GapMinder has hundreds of indicators of global well-being, health, and the environment.
* Our World in Data has a variety of biodiversity datasets.
* HYDE has a wide variety historical data covering the entire Holocene (12000 years or so).
* Pangeo is a community promoting open science and has preprocessed climate and weather datasets.

There are also numerous space, environmental and atmospheric datasets available from governments around the world.


## Sensors

[FieldKit](https://www.fieldkit.org/)

Notes:
If you want to collect data yourself, then you'll want to look into FieldKit, which includes an opensource platform for research grade hardware, a software platform for handling that data and community to help with the process.

Sensor kits start around $150 USD but a full set of weather sensors is up to $400 USD. A weather FieldKit station can record temperature, relative humidity, barometric pressure, wind speed, wind direction, and rainfall. They also have kits of water measurements and soon air quality measurements. Due to pandemic supply issues it could be hard to source these kits currently, but they remain one of the best open, artist-accessible sensor packages.

There is a growing community of conservationists trying to make opensource sensor and data tools. Check out [WildLabs](https://wildlabs.net/) and [Conservify](http://conservify.org/) online.

### Credits
* https://www.fieldkit.org/
* https://wildlabs.net/
* http://conservify.org/


## Formatting

Generally when using the Python language and popular ML time series tools you will want to save your data in CSV format. All spreadsheet software will be able to export to this format, so you can use any office software to manage the data until you export it for ML tools.

---

# Image data

Notes:
Image data is self-explanatory, but there are some important details. The format of the data can matter. Although it takes far larger amounts of space, PNG formatted images suffer from less compression artifacts than JPEGs. The vast majority of image data is still using JPEG format, but it won't help to save JPEGs as PNG formatted images since the data loss caused by compression happens when first saved as a JPEG. If you are creating your own images, use PNG or WEBP formatted images. WEBP has high quality and smaller file sizes, but as of yet is relatively unused.


## Scraping

Notes:
Many software artists use programs to "scrape" images from sources like Flickr. There are a number of issues with this, both technically and with copyright licences. It is possible however to download Creative Commons licensed images (that allow for use with or without attribution) from Flickr. Remember to keep a list of the image URLs you downloaded if you are publicly releasing the dataset so that you can respect any attribution requirements.

When collecting images from the Internet I recommend learning enough Python programming to run some scripts that will download the images for you given a particular search term. There are a few existing utilities that you can use, but you will often have to make small modifications to them and at minimum understand how to install Python and run python programs. Unfortunately, that is beyond the scope of this tutorial.

TODO: my script, links to scraping tutorials


[Openverse](https://wordpress.org/openverse/)
[Flickr CC](https://www.flickr.com/search/?text=&license=2%2C3%2C4%2C5%2C6%2C9&media=photos)
[Rawpixel PD](https://www.rawpixel.com/category/53/public-domain)
[Pexels PD](https://www.pexels.com/public-domain-images/)
[Unsplash](https://unsplash.com/)

Notes:
You can also collect images manually, although be prepared to invest a lot of time. There are a number of sites that you can use for public domain or Creative Commons (CC) licensed images. You can also use Google image search and their Creative Commons licensed image search option, but the images often are not verifiably licensed in any way.


## Preparing

Notes:
Images may have watermarks, frames, and other extraneous information in them. If you are using them in a generative model, any extraneous image elements should be cropped out or removed or the model may learn to generate them as well.

The main challenge with images is resolution and/or dimension. Most models require a particular resolution of image to train on and usually a square resolution (i.e. width and height are the same) that is relatively small. 1024 x 1024 is quite large for contemporary training.

Most models train on square images, but most images aren't square. This means images will be cropped, either on the fly during training, or during dataset preparation. I tend to favour controlling the image scaling and crops manually for smaller datasets. Manual crops can just center the subject in the square frame and then scale that crop to the appropriate size. This usually means that the left and right edges of your images will get cropped out, so beware of images where the subject is far to the left or right.

For video datasets you may be required to export the video into individual images and then process those.


## Existing

<div class="small">

* [VisualData Discovery](https://visualdata.io/discovery) computer vision dataset search
* [ImageNet](http://image-net.org/) the original labelled image dataset
* [Open Images](https://storage.googleapis.com/openimages/web/index.html) Google's labelled, segmented image dataset
* [COCO](https://cocodataset.org/) Microsoft's segmented, captioned, mostly labelled images
* [IMDB-WIKI](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/) face images with age and gender labels
* [Cityscapes](https://www.cityscapes-dataset.com/) labelled, segmented cityscape images
* [XView Dataset](http://xviewdataset.org/#dataset) annotated overhead imagery
* [LHQ](https://github.com/universome/alis) high quality landscape photos

Art:
* [Wiki Art](https://www.kaggle.com/c/painter-by-numbers/data) from [WikiArt](https://www.wikiart.org/) both public domain and copyrighted images of paintings, photographs and scultpures
* [Art Institute Chicago](https://www.artic.edu/collection?is_public_domain=1) public domain high resolution digitized collection (no dataset yet)

Video:
* [YouTube-8M](https://research.google.com/youtube8m/) labelled video dataset
* [Kinetics](https://deepmind.com/research/open-source/kinetics) video clips that cover human action classes

3D:
[CO3D: Common Objects In 3D](https://github.com/facebookresearch/co3d)

</div>

There are wealth of existing image datasets. Beyond ImageNet, the grandmother of labelled image datasets (which is currently undergoing improvements) there are Google's Open Images and Microsoft's COCO dataset. 

For landscape and aerial photography the LHQ and XView datasets are available, respectively, as well as the Cityscapes dataset for urban scenes.

Many models have been made from the Wikiart image collection and the Art Institute Chicago now has an astounding collection of high quality art scans available through an opensource API, although I haven't seen a downloadable dataset.

There are some video datasets available as well, based off Youtube videos, including the Kinetics dataset that focuses on human actions.

Many of these datasets are hundreds of gigabytes in size, so be prepared for long downloads and massive storage requirements.


## Augmentation

When training models using images care must be taken to avoid common problems with neural nets. 

For example, researchers found that texture was the most important attribute being used to make the predictions in early image classification models, but they could force more robust learning by adding silhouettes, line drawings, noise, clipping and partial masking of the image and other transformations to the training data to expand the range of characteristics that led to a classification. This data augmentation is critical now in image-based training.

Most augmentation is now included in computer vision related research projects and done automatically, on-the-fly during training, but be sure that you have it enabled.


This augmentation includes: 

* color, contrast, gamma, brightness shifts
* blur, noise and other effects
* rotation, scaling, reflection, & perspective shifts
* random cropping
* random cut out / erasing
* mixup: blending of two images
* style transfer

Automated image augmentation is an active area of research and tools are improving rapidly.

### Credits
* https://github.com/AgaMiko/data-augmentation-review

---
# Text data

There are quite a few text datasets, although in general unless you are interested in exploring text like Allison Parish, who uses text data similar to how image and time-series data is used, I would suggest using existing pre-trained large language models. It is possible to fine-tune these existing models on more specific vocabulary, although the training may be difficult or expensive.


## Existing large language models

* [OpenAI GPT](https://beta.openai.com/overview)
* [Hugging Face models](https://huggingface.co/models)

If you want the most effective and easiest solution, but by far the most expensive, OpenAI's GPT API is in beta right now. It can be fine-tuned and offers completion, question answering, classification, summarization, and semantic search amongst many other capabilities.

Hugging Face offers a number of pretrained large language models, but using them requires the capability of running them and doing any extra fine-tuning training yourself.

### Credits
* https://beta.openai.com/examples
* https://huggingface.co/models


# Audio

Speech:
* [Common Voice](https://voice.mozilla.org/en/datasets) open source multi-language dataset of voices
* [Open Speech and Language Resources](http://www.openslr.org/resources.php) particularly the [LibriSpeech](http://www.openslr.org/12/) a 1000 hour dataset of read English
* [The Spoken Wikipedia Corpora](https://nats.gitlab.io/swc/) hundreds of hours of aligned audio to text in multiple languages

Other audio:
* [AudioSet](https://research.google/tools/datasets/audioset/) labelled 10 second sound clips from YouTube videos
* [NSynth](https://magenta.tensorflow.org/datasets/nsynth) musical notes from a thousand synthesized instruments
* [Free Music Archive](https://freemusicarchive.org/) free music search engine (no dataset)

The majority of audio datasets are for natural language text-to-speech learning. These have limited use to artists generally, but there are a few well known non-voice datasets.


# Data pitfalls

We have covered some issues with data in the previous tutorials, including bias, diversity, and fairness. We'll do a quick review with a focus on what can be done to avoid or counteract these issues.


## Diversity and Bias

Datasets reflect the biases of the society from which they originate. As mentioned in the last tutorial, datasets gathered from the Internet lack diversity and have an over-representation of certain viewpoints, particularly, younger and WEIRD (Western, educated, industrialized, rich and democratic). Western biases, such as stereotypes about gender, minorities, and capitalism are pervasive.

Trying to counteract the lack of diversity can be challenging, for example, take this selection of search results for "construction worker" that attempts to balance gender diversity. However, the masculine-presenting individuals are more realistic, modern and active than the nostalgic, toy, non-realistic, passive and/or sexualised feminine-presenting individuals.

Thus, if you incorporate image datasets from the Internet, and are not critiquing them by presenting their flaws, you will need to carefully curate the dataset to manage the diversity and bias issues. This curation requires a vast amount of manual labour, but it is also very valuable, helping to create more freely available higher quality datasets for further research and other art.

Unfortunately, removing problematic images from datasets may not be enough to get sufficient diversity. You may also need to create or find images to bolster minority representations. Creating your own images, especially from minority groups, requires careful consideration of representation and consent.

### Credits <!-- -->
* [Measuring Diversity - Google PAIR](https://pair.withgoogle.com/explorables/measuring-diversity/)


Data based on historical trends is going to necessarily reflect the bias of that society at that time. In this fictional example of college GPA prediction, the more data points for each student, including gender and other attributes thought to be unrelated to GPA, the better the model predicts based on its historic data. Historical bias against women can cause the model to predict lower GPAs for women and if that bias remains in the educational institution a biased model may be more accurate. In essence a biased but accurate model can be used to detect bias in the systems its data is derived from, hopefully leading to reforms.

Hiding attributes that the model shouldn't use for its prediction is not fail-safe either. Models, like humans, can infer private attributes from public ones. For example, using a person's birthplace or living location to infer race or ethnicity.

Fortunately, in artistic practice your datasets will not be making consequential decisions or predictions about others. Nonetheless, understanding how your datasets may be biased is critical to the underlying themes and construction of your work.

### Credits <!-- -->
* [Hidden Bias - Google PAIR](https://pair.withgoogle.com/explorables/hidden-bias/)


# Making things ethically

Sadly, there are not yet many established best practices around ethical dataset creation in the Machine Learning community, especially for art projects. A majority of the existing work has been focused on results not ethics. Artists have had an overwhelming amount of technical issues to overcome and limited time and money to spend on creating and curating datasets. Instead, like many ML researchers, they have relied on existing research-focused datasets or have quickly hacked together datasets scraped from image hosting sites like Flickr.

Properly building the relationships and tools needed to collect, curate and share datasets in an ethical 'Good Way' is the current and ongoing responsibility of artists hoping to make art with data. So I would urge you to try to improve your practice with each work and share your tools and experiences with others.

There are many fields with related challenges including museum, library and information sciences, art history as well as other preservation, conservation and knowledge keeping practices. Also relevant are the practice of ethics and data management used in journalism and science research, particularly experimental studies. Plus all the open source and open data movements. Don't be shy to reach out for help and integrate best practices.

I wish there was more I could offer, but I too am at the start of this journey, so I am mainly able point out the footsteps of pioneers for us both to follow.


## FAIR, CARE & Local Contexts

In the Neural Nets and Data tutorial I introduced the open data FAIR principles, the CARE Indigenous principles, and the Local Contexts project for culturally appropriate access to cultural heritage and Indigenous data.

I will quickly review this and then suggest some practical steps to implement them:


FAIR:
* Findable
* Accessible
* Interoperable
* Reusable

CARE:
* Collective Benefit
* Authority to Control
* Responsibility
* Ethics

FAIR principles are findable, accessible, interoperable, and reusable data. In other words, place good signposts to your data, access it through standard doors and locks, storing it on standard sized paper with good labels, and note everything in an index.

CARE principles add community benefit from the data, getting consent from the community and inviting them to be stewards of the data, a responsibility to share how the data will be used, and holistic data ethics that includes the community's wellbeing at all stages of the data life cycle.

Let's explain this with an example dataset of collected and new photos of cultural heritage and art from a particular community and the land they live on.


Following Suzanne Kite's _How to Build Anything Ethically_ questions, we  can start a data project by identifying and connecting with elders and knowledge keepers of the community. They will help to identify other stakeholders, both human and non-human, past and present.

Building a coalition of stakeholders involves explaining the goals of the project and inviting participation and feedback. This is the opportunity to build relationships founded on respect, reciprocity, trust and mutual understanding. You can use open source practices developing the roadmap and documentation of the process in an open, collaborative way. As soon as possible you should have mock-ups of the final work and descriptions or mock-ups of how access to the dataset would work for the community to help everyone imagine the future you are working towards together. 

This roadmap should address the consensual building of the dataset, how it will be accessible to the community, and how you intended to use and publish it. You should also detail how the community can participate and how that will be acknowledged. What labour is required and what contribution is optional and how that contribution will be accepted.

If the data is coming from Indigenous sources, like in this example, you'll want to go through the [GIDA CARE Principles](https://www.gida-global.org/care) and address each one.

In this example, timelines for getting access to existing art and cultural works to photograph them and well as guided tours of the geography would be necessary. It may be necessary to photograph 3-4 times at different times of the year to incorporate all seasonal changes, and certain cultural artifacts may only be available seasonally. Visits to various community and personal dwellings will likely need to be coordinated, compensated and acknowledged. Keep track of everything, detailed notes will save you countless hours later! You can, with consent, also record audio and/or video of the data collection process, which can be consulted later to confirm the notes or catch mistakes.

There may be data that is requested to remain private to just the community or even certain parts of the community, so each data point and photo should have a list of metadata associated with it. This metadata should encompass current and potential future use cases. We can use the Local Context tags to identify cultural data practices. Using a tool like OpenRefine can help with metadata management. 

Data formats and all tools needed for access and contribution to the dataset should be free and open, so no additional payments are required.

### Credits
* https://www.gida-global.org/care
* https://kitekitekitekite.com/portfolio/items/indigenous-protocols-and-artificial-intelligence-position-paper/


## The Art of Maintenance

One important step often overlooked in art and tech practice is designing for maintenance and end of life. End of life for data requires decisions about data retention and deletion, but also encompasses recycling or reuses of hardware. In this section I am going to focus on maintenance.


Maintenance is a drag; it takes all the fucking time (lit.) The mind boggles and chafes at the boredom. The culture confers lousy status on maintenance jobs = minimum wages, housewives = no pay. 

Artist Mierle Laderman Ukeles manifesto lays out an approach where Care, i.e. the labour of maintenance, is Art. It is worth reading the entire manifesto, and I think the devaluing of maintenance work, especially due to its association with gendered labour is important to understand, but I want to emphasize the most practical aspect: maintenance takes all the time.

David Graeber, who was an anthropologist and anarchist activist, points this out too in his work on the care economy:


But a teacup or a bottle, well you know, you produce a cup once. You wash it like ten thousand times. Most work isn’t actually about producing new things, it’s about maintaining things.

Service to others is a type of care work. Production and maintenance of things that meet others needs and improve others freedom is a type of caring for others.

A parent takes care of a child, so that that child can grow and be healthy and flourish. That’s true. But in an immediate level, you take care of a child so the child can go and play. That’s what children actually do when you’re taking care of them. What is play? Play is like action done for its own sake. It’s in a way the very paradigm of freedom. Because action done for its own sake is what freedom really consists of. Play and freedom are ultimately the same thing.

Art too is for its own sake. Thus your dataset when used for art is about creating freedom, first for yourself and then for everyone else who has access to it. That is why it is critical to make it in an open and ethical way. This concept of care work as freedom generating is also important to Indigenous feminism.

But it is not just the creation of the dataset but the maintenance of that data that is critical for maximizing freedom. Care must be taken to minimize the maintenance costs and consider how to sustain the maintenance work of the dataset. When you are designing your project, all other things being roughly equal, choose the option that is best to maintain. Remember, the vast majority of the time and effort required and the freedom generated by the project is through maintenance.


Sadly there are few tools and even fewer sources of funding for maintenance work for art projects, unless your work is accepted by a major cultural institution into their permanent collection. Any project, art or otherwise, that uses computer hardware and software also rapidly becomes a maintenance nightmare. The cultural changes needed to prioritize maintenance and care over development and production are slow, but in the meantime we'll do what we can.

### Credits
* https://kortina.nyc/notes/n/manifesto-for-maintenance-art-1969/
* http://opentranscripts.org/transcript/managerial-feudalism-revolt-caring-classes/
* https://en.wikipedia.org/wiki/David_Graeber


### DisCo

One of the best examples of a successful care-first model is the [distributed cooperative organization](https://disco.coop/), the [Guerrilla Media Collective](https://disco.coop/labs/guerrilla-media-collective/). They have documented their practice and created a wonderful manifesto in collaboration with the [Transnational Institute](https://www.tni.org/), to help others learn about distributed cooperative organizations or DisCos.

Their documents detail a number of important principles, but there are three that I want to highlight here:


**Care work is the core**: organizations and collaborations are living entities on two scales: the entity and the members that constitute it. Both need care work to maintain their health and well-being.

**Put your effort where your heart is**: use values-based accountability. Production is guided not by profit but by social and environmental priorities.

**Building whole-community governance**: extend decision-making and stewardship to all contributors and those affected by your organization's actions.

These principles reflect the similar ideals we have seen in other feminist economic movements. Care work at the core makes clear that every collaboration requires maintenance and sustenance of both the collaboration and all the contributors to the collaboration, both human and non-human, animate and inanimate. 

These DisCo principles work well in dataset collaborations with diverse sets of contributors with different needs. Put care work at the core, guided by your shared values, for all contributors and those affected by your decisions and actions.

### Links
* https://disco.coop/
* https://disco.coop/labs/guerrilla-media-collective/
* https://wiki.guerrillamediacollective.org/index.php/Main_Page
* https://wiki.guerrillamediacollective.org/index.php/Working_Circles
* https://www.tni.org/


### Tools

What sort of tools are needed to manage maintenance and care work?

The Guerrilla Media Collective uses browser-based software and services, which allows for a single piece of software with various interfaces customized to the specific task at hand.

Beyond project specific tools, they use 6 different types of tools for all projects:

1. Clock/timer to track work (punch clock)
2. Synchronous group and individual communication (cafeteria or desk chat)
3. Asynchronous communication / decision-making (boardroom)
4. Task and project management (whiteboard and planner)
5. Collaborative writing and file-storage (file cabinet)
6. Wiki documentation (public report)
 
I would recommend all open source software for these choices and the companion document has more details on this, but options can be limited based on access to technical help and Guerrilla Media Collective has chosen online services that are not open source for some of their tools.

### Credits
* https://wiki.guerrillamediacollective.org/Category:Tools

---

[Data in Practice companion document](./companion.html)

I've put together a document that goes into further details on all the information we've covered in this tutorial. You can find it at this link.

---
<!-- .slide: data-audio-src="../audio/data/TODO.ogg" data-background-image="../images/Five_Directions_dark.webp" data-background-opacity="0.9" data-audio-advance="800" -->
# Thank you

<div class="backdrop">

1. [Foundations](../foundations/) <!-- .element: class="lighten" -->
2. [Past, Present, Future](../past_present_future/) <!-- .element: class="lighten" -->
3. [Neural Nets](../neural_nets/) <!-- .element: class="lighten" -->
4. **Data in Practice**
5. ***Machine Learning Art*** 

</div>

Notes:
Well, that's the end of the fourth tutorial. Thank you for your attention. I hope you'll check out the next in the series; Machine Learning Art, where we'll look at specific art projects and artists and how they have adapted ML into their practice, examples of popular tools, and consider some project ideas.

See you there!


# Review

1. Foundations
2. Past, Present and Future
3. Neural Nets
4. **Data in Practice**
5. Machine Learning Art

Notes:
In this presentation we'll talk about the practical aspects of data collection. Where to find existing datasets, how to preprocess data, the different types of datasets, and how to avoid some of the pitfalls we discussed previously.

In the first tutorial, Foundations, we went over the principles and foundations needed to understand the philosophy and importance of machine learning art. In the second, Past, Present, Future we looked at the historical context and explored the origins of machine learning art, to see how the technology was developed, how it was thought of and shaped by its creators and how it influenced the world. We looked at the current start of the art and imagined some possible futures. In part 3, Neural Nets we covered how neural nets are constructed, trained and operated. We looked at the current problems neural nets face. All neural nets require data to train on, and we investigated the ethics of data collection and appropriation.



### Foundations
* Digital: noise, copies and ethics
* Software: abstraction and neural nets
* Intelligence and digital intelligence

Notes:
In Foundations we started with a basic understand of digital technology - a battle against noise to create a strange digital world of perfect, cheap copies that radically changes what is possible to compute. We discussed the ethical implications of this. We got to know some of the basic principles that underlie all software and how neural net software, that excels at pattern recognition, differs from traditional programmed software. Then looked at different ways of thinking about intelligence: narrow versus general, conscious versus nonconscious cogition, and stupidity versus good judgment. Finally, we talked about what happens when intelligence becomes digital and those implications.



### Past, Present, Future
* Exponential technology development
* Past
  * Symbolic vs cybernetic
  * Neural nets survive two winters
* Present
  * Assistants, translation, generative models
  * Surveillance, whales, and automation
* Future:
  * Neuro-symbolic AI
  * Better, more general learning with less data
  * Art vs entertainment

Notes:
Exponential phenomenon, which technology development trends tends to follow, are surprising to human thinking. During the 20th century the advent of electronic and digital technology gave rise to the hopes of both symbolist and cybernetic adherents to develop machine intelligence. After numerous setbacks a combination of cheaper, faster computation and large datasets due to the Internet finally allowed the advancement of neural network based techniques to damatically out perform other approaches.

In the present day, we are surrounded by assistant, translation, recommendation, and moderation technologies that are used to continue the progression of a monopolistic business environment dependent on surveillance and advertising. We did a quick look at popular generative models used by artists in the last few years.

I ended with some predictions about the future. More complex artificial neurons and modular, composable dynamic structures combined with improved training will reduce the amount of data needed with better generalization and more robustness. Video generation will become possible make art making, direction and curation, more accessible. AI tools will be able to create content for a single person.



### Neural nets and data

* Perceptrons
* Data, Model, Training in neural network
* Unsupervised, Supervised, Reinforcement learning
* Recurrent networks
* Convolutional networks
* Features and latent spaces
* Deep learning challenges
* Data ethics and appropriation

We reviewed the Perceptron, from 1960 to understand the basic operations of a neural net. Then looked at the three main components of all neural nets: data, model, and training. Networks can learn in an unsupervised manner with unlabelled data, while supervised learning uses labelled data. Reinforcement learning uses a reward signal for agents that can interact with their environments.

We looked at commonly used and important types of networks, the recurrent network that has looped connections and convolutional networks that are commonly used for image processing. 

We discussed features and the latent spaces of models. Then we covered existing challenges in deep learning, some of which we'll talk about overcoming in this tutorial.

Finally, we discussed ethics around data collection which will give us context with some specific practices we'll talk about next.

